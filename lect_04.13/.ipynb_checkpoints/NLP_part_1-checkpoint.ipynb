{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Pictures/NLP.png)\n",
    "\n",
    "# Анализ текстовых данных\n",
    "\n",
    "Natural Language Processing https://habr.com/ru/company/Voximplant/blog/446738/\n",
    "\n",
    "Natural Language Processing (далее – NLP) – обработка естественного языка – подраздел информатики и AI, посвященный тому, как компьютеры анализируют естественные (человеческие) языки. NLP позволяет применять алгоритмы машинного обучения для текста и речи.\n",
    "\n",
    "Например, мы можем использовать NLP, чтобы создавать системы вроде распознавания речи, обобщения документов, машинного перевода, выявления спама, распознавания именованных сущностей, ответов на вопросы, автокомплита, предиктивного ввода текста и т.д.\n",
    "\n",
    "Сегодня у многих из нас есть смартфоны с распознаванием речи – в них используется NLP для того, чтобы понимать нашу речь. Также многие люди используют ноутбуки со встроенным в ОС распознаванием речи.\n",
    "\n",
    "### Какие задачи можно решать, обрабатывая текст?\n",
    "\n",
    "1. синтаксические задачи\n",
    "  * разметка по частям речи \n",
    "  * деление слов в тексте на морфемы (суффикс, приставка и пр.)\n",
    "  * стемминг, лемматизация \n",
    "  * деление на предложения (инициалы и сокращения) и слова (китайский язык)\n",
    "  * поиск имен и названий в тексте - сущностей (named entity recognition)\n",
    "  \n",
    "2. задачи на понимание текста, в которых есть \"учитель\"\n",
    "  * предсказание следующего символа\n",
    "  * информационный поиск\n",
    "  * анализ тональности\n",
    "  * выделение отношений и фактов\n",
    "  * ответы на вопросы\n",
    "  \n",
    "3. понимание и порождение текста \n",
    "  * порождение текста\n",
    "  * машинный перевод\n",
    "  * диалоговые модели (чат-бот)\n",
    "  \n",
    "  \n",
    "  * распознавание речи (помощник)\n",
    "  * чат-бот (замена техподдержки в решении большинства вопросов)\n",
    "  * поиск точного ответа на вопрос в базе документов (например, база стандартов)\n",
    "  * оценка мнения в социальных сетях о продукте\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_ NLTK (Natural Language Toolkit) – ведущая платформа для создания NLP-программ на Python. У нее есть легкие в использовании интерфейсы для многих языковых корпусов, а также библиотеки для обработки текстов для классификации, токенизации, стемминга, разметки, фильтрации и семантических рассуждений. _\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()  # download lots of data at first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# От текста к простым моделям"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Подготовка текста к обработке. Подход Bag-of-words model (мешок слов). \n",
    "\n",
    "* Разбиваем текст на слова (токены) \n",
    "* Избавляемся от грамматических форм\n",
    "* Убираем стоп-слова (местоимения, предлоги и т.п.)\n",
    "* Множество слов = множество признаков. Переводим признаки в числа (например, частота слова)\n",
    "\n",
    "Применение стандартных методов к модели. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разбиение на токены\n",
    "**Def.**  \n",
    "разбиение последовательности символов на части (токены), возможно, исключая из рассмотрения некоторые символы  \n",
    "Наивный подход: разделить строку пробелами и выкинуть знаки препинания  \n",
    "\n",
    "\n",
    "*Трисия любила Нью-Йорк, поскольку любовь к Нью-Йорку могла положительно повлиять на ее карьеру.*  \n",
    "\n",
    "\n",
    "**Проблемы:**  \n",
    "* my.email@mail.ru, 127.0.0.1\n",
    "* С++, C#\n",
    "* York University vs New York University\n",
    "* Зависимость от языка (“Lebensversicherungsgesellschaftsangestellter”, “l’amour”)\n",
    "Альтернатива: n-граммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text=\"\"\"\n",
    "Backgammon is one of the oldest known board games. \n",
    "Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East. \n",
    "It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Backgammon is one of the oldest known board games.\n",
      "\n",
      "Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East.\n",
      "\n",
      "It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(text)\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Backgammon', 'is', 'one', 'of', 'the', 'oldest', 'known', 'board', 'games', '.']\n",
      "\n",
      "['Its', 'history', 'can', 'be', 'traced', 'back', 'nearly', '5,000', 'years', 'to', 'archeological', 'discoveries', 'in', 'the', 'Middle', 'East', '.']\n",
      "\n",
      "['It', 'is', 'a', 'two', 'player', 'game', 'where', 'each', 'player', 'has', 'fifteen', 'checkers', 'which', 'move', 'between', 'twenty-four', 'points', 'according', 'to', 'the', 'roll', 'of', 'two', 'dice', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Токенизация по словам\n",
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    print(words)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#Стопслова \n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Backgammon', 'one', 'oldest', 'known', 'board', 'games', '.']\n"
     ]
    }
   ],
   "source": [
    "#Убираем стоп-слова\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "sentence = \"Backgammon is one of the oldest known board games.\"\n",
    "\n",
    "words = nltk.word_tokenize(sentence)\n",
    "without_stop_words = [word for word in words if not word in stop_words]\n",
    "print(without_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Наборы из стоп слов не только на английском\n",
    "from nltk.corpus import stopwords\n",
    "print (' '.join(stopwords.words('russian')[:20]))\n",
    "print (' '.join(stopwords.words('english')[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'development', 'snowboarding', 'inspired', 'skateboarding', ',', 'sledding', ',', 'surfing', 'skiing', '.']\n"
     ]
    }
   ],
   "source": [
    "#Что делать со знаками препинания?\n",
    "sentence = \"The development of snowboarding was inspired by skateboarding, sledding, surfing and skiing.\"\n",
    "\n",
    "words = nltk.word_tokenize(sentence)\n",
    "without_stop_words = [word for word in words if not word in stop_words]\n",
    "print(without_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "development\n",
      "of\n",
      "snowboarding\n",
      "was\n",
      "inspired\n",
      "by\n",
      "skateboarding\n",
      ",\n",
      "sledding\n",
      ",\n",
      "surfing\n",
      "and\n",
      "skiing\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "#Использование регулярных выражений \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer('\\w+|[^\\w\\s]+')\n",
    "\n",
    "for t in tokenizer.tokenize(sentence ): \n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The development of snowboarding was inspired by skateboarding  sledding  surfing and skiing \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = r\"[^\\w]\"\n",
    "print(re.sub(pattern, \" \", sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Регулярное выражение__ (регулярка, regexp, regex) – это последовательность символов, которая определяет шаблон поиска. Например:\n",
    "\n",
    "    . – любой символ, кроме перевода строки;\n",
    "    \\w – одно слово;\n",
    "    \\d – одна цифра;\n",
    "    \\s – один пробел;\n",
    "    \\W – одно НЕслово;\n",
    "    \\D – одна НЕцифра;\n",
    "    \\S – один НЕпробел;\n",
    "    [abc] – находит любой из указанных символов match any of a, b, or c;\n",
    "    [^abc] – находит любой символ, кроме указанных;\n",
    "    [a-g] – находит символ в промежутке от a до g.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нормализация\n",
    "\n",
    "Приведение токенов к единому виду для того, чтобы избавиться от поверхностной разницы в написании  \n",
    "\n",
    "Подходы  \n",
    "* сформулировать набор правил, по которым преобразуется токен  \n",
    "Нью-Йорк → нью-йорк → ньюйорк → ньюиорк\n",
    "* явно хранить связи между токенами (WordNet – Princeton)  \n",
    "машина → автомобиль, Windows 6→ window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "нью-йорк\n"
     ]
    }
   ],
   "source": [
    "s = 'Нью-Йорк'\n",
    "s1 = s.lower()\n",
    "print(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ньюйорк\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "s2 = re.sub(\"\\W\", \"\", s1)\n",
    "print(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ньюиорк\n"
     ]
    }
   ],
   "source": [
    "s3 = re.sub(\"й\", u\"и\", s2)\n",
    "print(s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стемминг и Лемматизация\n",
    "\n",
    "Приведение грамматических форм слова и однокоренных слов к единой основе (lemma):\n",
    "\n",
    "* Stemming – с помощью простых эвристических правил\n",
    "  * Porter (Cambridge – 1980)\n",
    "        5 этапов, на каждом применяется набор правил, таких как\n",
    "            sses → ss (caresses → caress)\n",
    "            ies → i (ponies → poni)\n",
    "\n",
    "  * Lovins (1968)\n",
    "  * Paice (1990)\n",
    "  * другие\n",
    "* Lemmatization – с использованием словарей и морфологического анализа\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стемминг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token\n",
      "stem\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import PorterStemmer\n",
    "s = PorterStemmer()\n",
    "print (s.stem('Tokenization'))\n",
    "print (s.stem('stemming'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "авиац\n",
      "национальн\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import RussianStemmer\n",
    "r = RussianStemmer()\n",
    "print(r.stem('Авиация'))\n",
    "print(r.stem('национальный'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лемматизация \n",
    "(обычно лучше работает для сложных языков, в том числе для русского)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: Parse(word='ключ', tag=OpencorporaTag('NOUN,inan,masc sing,nomn'), normal_form='ключ', score=0.5, methods_stack=((<DictionaryAnalyzer>, 'ключ', 134, 0),))\n",
      "Word: ключ | Normal form: ключ\n",
      "\n",
      "\n",
      "Metadata: Parse(word='ключ', tag=OpencorporaTag('NOUN,inan,masc sing,accs'), normal_form='ключ', score=0.5, methods_stack=((<DictionaryAnalyzer>, 'ключ', 134, 3),))\n",
      "Word: ключ | Normal form: ключ\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pymorphy2\n",
    "\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "for i in morph.parse(u'ключ'):\n",
    "    print(\"Metadata: {}\".format(i)) \n",
    "    print(\"Word: {} | Normal form: {}\".format(i.word, i.normal_form))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Представление документов - Bag of Words\n",
    "\n",
    "**Boolean Model.** Присутствие или отсутствие слова в документе  \n",
    "**Bag of Words.** Порядок токенов не важен  \n",
    "\n",
    "*Погода была ужасная, принцесса была прекрасная.\n",
    "Или все было наоборот?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Перевод текста в матрицу, свойства - различные слова, значение их частота в тексте \n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 0., 1.],\n",
       "       [0., 1., 3.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvectorizer = DictVectorizer(sparse=False)\n",
    "text_dict = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}]\n",
    "X = dvectorizer.fit_transform(text_dict)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bar': 2.0, 'foo': 1.0}, {'baz': 1.0, 'foo': 3.0}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvectorizer.inverse_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 4.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvectorizer.transform({'foo': 4, 'unseen_feature': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bar', 'baz', 'foo']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvectorizer.feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bar</th>\n",
       "      <th>baz</th>\n",
       "      <th>foo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bar  baz  foo\n",
       "0  2.0  0.0  1.0\n",
       "1  0.0  1.0  3.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(X,columns=dvectorizer.feature_names_)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Counter({',': 1, '.': 1, 'mr': 1, 'president': 1, 'thank': 1}),\n",
       " Counter({\"'\": 1,\n",
       "          ',': 3,\n",
       "          ':': 1,\n",
       "          'agree': 1,\n",
       "          'auspicious': 1,\n",
       "          'european': 1,\n",
       "          'madam': 1,\n",
       "          'needs': 1,\n",
       "          'outcome': 1,\n",
       "          'president': 1,\n",
       "          'prospects': 2,\n",
       "          'recognise': 1,\n",
       "          'turkey': 2}),\n",
       " Counter({',': 2,\n",
       "          '.': 1,\n",
       "          'agenda': 1,\n",
       "          'early': 1,\n",
       "          'express': 1,\n",
       "          'firstly': 1,\n",
       "          'high': 1,\n",
       "          'important': 1,\n",
       "          'including': 1,\n",
       "          'issue': 1,\n",
       "          'like': 1,\n",
       "          'madam': 1,\n",
       "          'president': 1,\n",
       "          'representative': 1,\n",
       "          'sincerest': 1,\n",
       "          'stage': 1,\n",
       "          'thanks': 1,\n",
       "          'would': 1})]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ПОлучение словарей из текстов (ручной подход, см далее библиотечную функцию)\n",
    "from collections import Counter\n",
    "\n",
    "docs = [\n",
    "    \"Thank you, Mr President.\",\n",
    "    \"Madam President, I agree and recognise Turkey's European prospects, but if these prospects are to have an auspicious outcome, Turkey needs to:\",\n",
    "    \"Madam President, firstly, I would like to express my sincerest thanks to the High Representative for including this important issue in the agenda at such an early stage.\",\n",
    "]\n",
    "\n",
    "tokenizer = RegexpTokenizer('\\w+|[^\\w\\s]+')\n",
    "stopwords_eng = stopwords.words()\n",
    "\n",
    "document_bags = list()\n",
    "\n",
    "for d in docs:\n",
    "    bag = Counter()\n",
    "    text = d.lower()\n",
    "    for t in tokenizer.tokenize(text):     \n",
    "        if t in stopwords_eng:\n",
    "            continue\n",
    "        bag[t] += 1\n",
    "    document_bags.append(bag)\n",
    "    \n",
    "document_bags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Как выглядят данные в таблице?\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>'</th>\n",
       "      <th>,</th>\n",
       "      <th>.</th>\n",
       "      <th>:</th>\n",
       "      <th>agenda</th>\n",
       "      <th>agree</th>\n",
       "      <th>auspicious</th>\n",
       "      <th>early</th>\n",
       "      <th>european</th>\n",
       "      <th>express</th>\n",
       "      <th>...</th>\n",
       "      <th>president</th>\n",
       "      <th>prospects</th>\n",
       "      <th>recognise</th>\n",
       "      <th>representative</th>\n",
       "      <th>sincerest</th>\n",
       "      <th>stage</th>\n",
       "      <th>thank</th>\n",
       "      <th>thanks</th>\n",
       "      <th>turkey</th>\n",
       "      <th>would</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     '    ,    .    :  agenda  agree  auspicious  early  european  express  \\\n",
       "0  0.0  1.0  1.0  0.0     0.0    0.0         0.0    0.0       0.0      0.0   \n",
       "1  1.0  3.0  0.0  1.0     0.0    1.0         1.0    0.0       1.0      0.0   \n",
       "2  0.0  2.0  1.0  0.0     1.0    0.0         0.0    1.0       0.0      1.0   \n",
       "\n",
       "   ...  president  prospects  recognise  representative  sincerest  stage  \\\n",
       "0  ...        1.0        0.0        0.0             0.0        0.0    0.0   \n",
       "1  ...        1.0        2.0        1.0             0.0        0.0    0.0   \n",
       "2  ...        1.0        0.0        0.0             1.0        1.0    1.0   \n",
       "\n",
       "   thank  thanks  turkey  would  \n",
       "0    1.0     0.0     0.0    0.0  \n",
       "1    0.0     0.0     2.0    0.0  \n",
       "2    0.0     1.0     0.0    1.0  \n",
       "\n",
       "[3 rows x 30 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dvectorizer.fit_transform(document_bags), columns=dvectorizer.feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'\",\n",
       " ',',\n",
       " '.',\n",
       " ':',\n",
       " 'agenda',\n",
       " 'agree',\n",
       " 'auspicious',\n",
       " 'early',\n",
       " 'european',\n",
       " 'express',\n",
       " 'firstly',\n",
       " 'high',\n",
       " 'important',\n",
       " 'including',\n",
       " 'issue',\n",
       " 'like',\n",
       " 'madam',\n",
       " 'mr',\n",
       " 'needs',\n",
       " 'outcome',\n",
       " 'president',\n",
       " 'prospects',\n",
       " 'recognise',\n",
       " 'representative',\n",
       " 'sincerest',\n",
       " 'stage',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'turkey',\n",
       " 'would']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvectorizer.feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x25 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 28 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Получение словаря - библиотечная функция\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "sklearn_vectorizer = CountVectorizer(stop_words='english')\n",
    "sklearn_vectorizer.fit_transform(docs)\n",
    "#Представление в виде обычной матрицы (а не разреженной)\n",
    "#sklearn_vectorizer.fit_transform(docs).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agenda': 0,\n",
       " 'agree': 1,\n",
       " 'auspicious': 2,\n",
       " 'early': 3,\n",
       " 'european': 4,\n",
       " 'express': 5,\n",
       " 'firstly': 6,\n",
       " 'high': 7,\n",
       " 'important': 8,\n",
       " 'including': 9,\n",
       " 'issue': 10,\n",
       " 'like': 11,\n",
       " 'madam': 12,\n",
       " 'mr': 13,\n",
       " 'needs': 14,\n",
       " 'outcome': 15,\n",
       " 'president': 16,\n",
       " 'prospects': 17,\n",
       " 'recognise': 18,\n",
       " 'representative': 19,\n",
       " 'sincerest': 20,\n",
       " 'stage': 21,\n",
       " 'thank': 22,\n",
       " 'thanks': 23,\n",
       " 'turkey': 24}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Частота встречаемости слова в тексте не всегда дает хорошую характеристику важности слова для обучения. Учет важности слова в тексте. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "Количество вхождений слова $t$ в документе $d$\n",
    "$$\n",
    "TF_{t,d} = term\\!\\!-\\!\\!frequency(t, d)\n",
    "$$\n",
    "Количество документов из $N$ возможных, где встречается $t$\n",
    "$$\n",
    "DF_t = document\\!\\!-\\!\\!fequency(t)\n",
    "$$\n",
    "$$\n",
    "IDF_t = inverse\\!\\!-\\!\\!document\\!\\!-\\!\\!frequency(t) = \\log \\frac{N}{DF_t}\n",
    "$$\n",
    "TF-IDF\n",
    "$$\n",
    "TF\\!\\!-\\!\\!IDF_{t,d} = TF_{t,d} \\times IDF_t\n",
    "$$\n",
    "\n",
    "Оценивает важность слова в контексте документа, являющегося частью корпуса\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thank you, Mr President.',\n",
       " \"Madam President, I agree and recognise Turkey's European prospects, but if these prospects are to have an auspicious outcome, Turkey needs to:\",\n",
       " 'Madam President, firstly, I would like to express my sincerest thanks to the High Representative for including this important issue in the agenda at such an early stage.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Напомним пример\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 1.69314718, 0.        ,\n",
       "         0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 1.69314718, 0.        , 0.        ],\n",
       "        [0.        , 1.69314718, 1.69314718, 0.        , 1.69314718,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 1.28768207, 0.        , 1.69314718,\n",
       "         1.69314718, 1.        , 3.38629436, 1.69314718, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 3.38629436],\n",
       "        [1.69314718, 0.        , 0.        , 1.69314718, 0.        ,\n",
       "         1.69314718, 1.69314718, 1.69314718, 1.69314718, 1.69314718,\n",
       "         1.69314718, 1.69314718, 1.28768207, 0.        , 0.        ,\n",
       "         0.        , 1.        , 0.        , 0.        , 1.69314718,\n",
       "         1.69314718, 1.69314718, 0.        , 1.69314718, 0.        ]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', norm=None)\n",
    "features = vectorizer.fit_transform(docs).todense()\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thank': 22,\n",
       " 'mr': 13,\n",
       " 'president': 16,\n",
       " 'madam': 12,\n",
       " 'agree': 1,\n",
       " 'recognise': 18,\n",
       " 'turkey': 24,\n",
       " 'european': 4,\n",
       " 'prospects': 17,\n",
       " 'auspicious': 2,\n",
       " 'outcome': 15,\n",
       " 'needs': 14,\n",
       " 'firstly': 6,\n",
       " 'like': 11,\n",
       " 'express': 5,\n",
       " 'sincerest': 20,\n",
       " 'thanks': 23,\n",
       " 'high': 7,\n",
       " 'representative': 19,\n",
       " 'including': 9,\n",
       " 'important': 8,\n",
       " 'issue': 10,\n",
       " 'agenda': 0,\n",
       " 'early': 3,\n",
       " 'stage': 21}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agenda</th>\n",
       "      <th>agree</th>\n",
       "      <th>auspicious</th>\n",
       "      <th>early</th>\n",
       "      <th>european</th>\n",
       "      <th>express</th>\n",
       "      <th>firstly</th>\n",
       "      <th>high</th>\n",
       "      <th>important</th>\n",
       "      <th>including</th>\n",
       "      <th>...</th>\n",
       "      <th>outcome</th>\n",
       "      <th>president</th>\n",
       "      <th>prospects</th>\n",
       "      <th>recognise</th>\n",
       "      <th>representative</th>\n",
       "      <th>sincerest</th>\n",
       "      <th>stage</th>\n",
       "      <th>thank</th>\n",
       "      <th>thanks</th>\n",
       "      <th>turkey</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.386294</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     agenda     agree  auspicious     early  european   express   firstly  \\\n",
       "0  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  1.693147    1.693147  0.000000  1.693147  0.000000  0.000000   \n",
       "2  1.693147  0.000000    0.000000  1.693147  0.000000  1.693147  1.693147   \n",
       "\n",
       "       high  important  including  ...   outcome  president  prospects  \\\n",
       "0  0.000000   0.000000   0.000000  ...  0.000000        1.0   0.000000   \n",
       "1  0.000000   0.000000   0.000000  ...  1.693147        1.0   3.386294   \n",
       "2  1.693147   1.693147   1.693147  ...  0.000000        1.0   0.000000   \n",
       "\n",
       "   recognise  representative  sincerest     stage     thank    thanks  \\\n",
       "0   0.000000        0.000000   0.000000  0.000000  1.693147  0.000000   \n",
       "1   1.693147        0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "2   0.000000        1.693147   1.693147  1.693147  0.000000  1.693147   \n",
       "\n",
       "     turkey  \n",
       "0  0.000000  \n",
       "1  3.386294  \n",
       "2  0.000000  \n",
       "\n",
       "[3 rows x 25 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(features, columns=vectorizer.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Закончили обрабатывать текст. Теперь приступаем к построению простых моделей. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Дополнительно  чтение из файла и составление мешка слов (по частоте)\n",
    "\n",
    "#чтение из файла по строкам\n",
    "with open(\"simple movie reviews.txt\", \"r\") as file:\n",
    "    documents = file.read().splitlines()\n",
    "    \n",
    "print(documents)\n",
    "\n",
    "# Import the libraries we need\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Step 2. Design the Vocabulary\n",
    "# The default token pattern removes tokens of a single character. That's why we don't have the \"I\" and \"s\" tokens in the output\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Step 3. Create the Bag-of-Words Model\n",
    "bag_of_words = count_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Show the Bag-of-Words Model as a pandas DataFrame\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "pd.DataFrame(bag_of_words.toarray(), columns = feature_names)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "values = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Show the Model as a pandas DataFrame\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "pd.DataFrame(values.toarray(), columns = feature_names)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
